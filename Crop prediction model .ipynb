
CROP YIELD PREDICTION IN INDIA
Predicting yield helps the state to get an estimate of the crop in a certain year to control the price rates.This model focuses on predicting the crop yield in advance by analyzing factors like location, season, and crop type through machine learning techniques on previously collected datasets.

# importing necessary libraries 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# loading the dataset

crop_data=pd.read_csv("crop_production.csv")
crop_data
State_Name	District_Name	Crop_Year	Season	Crop	Area	Production
0	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Arecanut	1254.0	2000.0
1	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Other Kharif pulses	2.0	1.0
2	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Rice	102.0	321.0
3	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Banana	176.0	641.0
4	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Cashewnut	720.0	165.0
...	...	...	...	...	...	...	...
246086	West Bengal	PURULIA	2014	Summer	Rice	306.0	801.0
246087	West Bengal	PURULIA	2014	Summer	Sesamum	627.0	463.0
246088	West Bengal	PURULIA	2014	Whole Year	Sugarcane	324.0	16250.0
246089	West Bengal	PURULIA	2014	Winter	Rice	279151.0	597899.0
246090	West Bengal	PURULIA	2014	Winter	Sesamum	175.0	88.0
246091 rows × 7 columns

crop_data.shape

#rows X columns
(246091, 7)
# dataset columns
crop_data.columns
Index(['State_Name', 'District_Name', 'Crop_Year', 'Season', 'Crop', 'Area',
       'Production'],
      dtype='object')
# statistical inference of the dataset

crop_data.describe()
Crop_Year	Area	Production
count	246091.000000	2.460910e+05	2.423610e+05
mean	2005.643018	1.200282e+04	5.825034e+05
std	4.952164	5.052340e+04	1.706581e+07
min	1997.000000	4.000000e-02	0.000000e+00
25%	2002.000000	8.000000e+01	8.800000e+01
50%	2006.000000	5.820000e+02	7.290000e+02
75%	2010.000000	4.392000e+03	7.023000e+03
max	2015.000000	8.580100e+06	1.250800e+09
# Checking missing values of the dataset in each column
crop_data.isnull().sum()
State_Name          0
District_Name       0
Crop_Year           0
Season              0
Crop                0
Area                0
Production       3730
dtype: int64
# Dropping missing values 
crop_data = crop_data.dropna()
crop_data
State_Name	District_Name	Crop_Year	Season	Crop	Area	Production
0	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Arecanut	1254.0	2000.0
1	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Other Kharif pulses	2.0	1.0
2	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Rice	102.0	321.0
3	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Banana	176.0	641.0
4	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Cashewnut	720.0	165.0
...	...	...	...	...	...	...	...
246086	West Bengal	PURULIA	2014	Summer	Rice	306.0	801.0
246087	West Bengal	PURULIA	2014	Summer	Sesamum	627.0	463.0
246088	West Bengal	PURULIA	2014	Whole Year	Sugarcane	324.0	16250.0
246089	West Bengal	PURULIA	2014	Winter	Rice	279151.0	597899.0
246090	West Bengal	PURULIA	2014	Winter	Sesamum	175.0	88.0
242361 rows × 7 columns

#checking
crop_data.isnull().values.any()
False
# Displaying State Names present in the dataset
crop_data.State_Name.unique()
array(['Andaman and Nicobar Islands', 'Andhra Pradesh',
       'Arunachal Pradesh', 'Assam', 'Bihar', 'Chandigarh',
       'Chhattisgarh', 'Dadra and Nagar Haveli', 'Goa', 'Gujarat',
       'Haryana', 'Himachal Pradesh', 'Jammu and Kashmir ', 'Jharkhand',
       'Karnataka', 'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur',
       'Meghalaya', 'Mizoram', 'Nagaland', 'Odisha', 'Puducherry',
       'Punjab', 'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana ',
       'Tripura', 'Uttar Pradesh', 'Uttarakhand', 'West Bengal'],
      dtype=object)
# Adding a new column Yield which indicates Production per unit Area. 

crop_data['Yield'] = (crop_data['Production'] / crop_data['Area'])
crop_data.head(10) 
<ipython-input-10-21ef19bb9e83>:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  crop_data['Yield'] = (crop_data['Production'] / crop_data['Area'])
State_Name	District_Name	Crop_Year	Season	Crop	Area	Production	Yield
0	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Arecanut	1254.0	2000.0	1.594896
1	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Other Kharif pulses	2.0	1.0	0.500000
2	Andaman and Nicobar Islands	NICOBARS	2000	Kharif	Rice	102.0	321.0	3.147059
3	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Banana	176.0	641.0	3.642045
4	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Cashewnut	720.0	165.0	0.229167
5	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Coconut	18168.0	65100000.0	3583.223250
6	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Dry ginger	36.0	100.0	2.777778
7	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Sugarcane	1.0	2.0	2.000000
8	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Sweet potato	5.0	15.0	3.000000
9	Andaman and Nicobar Islands	NICOBARS	2000	Whole Year	Tapioca	40.0	169.0	4.225000
# Visualizing the features

ax = sns.pairplot(crop_data)
ax
<seaborn.axisgrid.PairGrid at 0x7f454cef6d90>

# Dropping unnecessary columns

data = crop_data.drop(['State_Name'], axis = 1)
data.corr()
Crop_Year	Area	Production	Yield
Crop_Year	1.000000	-0.025305	0.006989	0.013499
Area	-0.025305	1.000000	0.040587	0.001822
Production	0.006989	0.040587	1.000000	0.330961
Yield	0.013499	0.001822	0.330961	1.000000
sns.heatmap(data.corr(), annot =True)
plt.title('Correlation Matrix')
Text(0.5, 1.0, 'Correlation Matrix')

dummy = pd.get_dummies(data)
dummy
Crop_Year	Area	Production	Yield	District_Name_24 PARAGANAS NORTH	District_Name_24 PARAGANAS SOUTH	District_Name_ADILABAD	District_Name_AGAR MALWA	District_Name_AGRA	District_Name_AHMADABAD	...	Crop_Turmeric	Crop_Turnip	Crop_Urad	Crop_Varagu	Crop_Water Melon	Crop_Wheat	Crop_Yam	Crop_other fibres	Crop_other misc. pulses	Crop_other oilseeds
0	2000	1254.0	2000.0	1.594896	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
1	2000	2.0	1.0	0.500000	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
2	2000	102.0	321.0	3.147059	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
3	2000	176.0	641.0	3.642045	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
4	2000	720.0	165.0	0.229167	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
246086	2014	306.0	801.0	2.617647	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
246087	2014	627.0	463.0	0.738437	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
246088	2014	324.0	16250.0	50.154321	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
246089	2014	279151.0	597899.0	2.141848	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
246090	2014	175.0	88.0	0.502857	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
242361 rows × 780 columns

Splitting dataset into train and test dataset

from sklearn.model_selection import train_test_split

x = dummy.drop(["Production","Yield"], axis=1)
y = dummy["Production"]

# Splitting data set - 25% test dataset and 75% 

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=5)

print("x_train :",x_train.shape)
print("x_test :",x_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)
x_train : (181770, 778)
x_test : (60591, 778)
y_train : (181770,)
y_test : (60591,)
print(x_train)
print(y_train)
        Crop_Year    Area  District_Name_24 PARAGANAS NORTH  \
201072       2013    16.0                                 0   
191897       1998  5400.0                                 0   
43814        2000  2968.0                                 0   
32815        2013   211.0                                 0   
62249        2006  1700.0                                 0   
...           ...     ...                               ...   
236131       2000   207.0                                 0   
127145       2007    39.0                                 0   
20536        2005    43.0                                 0   
18709        2011  2489.0                                 0   
35767        1999    67.0                                 0   

        District_Name_24 PARAGANAS SOUTH  District_Name_ADILABAD  \
201072                                 0                       0   
191897                                 0                       0   
43814                                  0                       0   
32815                                  0                       0   
62249                                  0                       0   
...                                  ...                     ...   
236131                                 0                       0   
127145                                 0                       0   
20536                                  0                       0   
18709                                  0                       0   
35767                                  0                       0   

        District_Name_AGAR MALWA  District_Name_AGRA  District_Name_AHMADABAD  \
201072                         0                   0                        0   
191897                         0                   0                        0   
43814                          0                   0                        0   
32815                          0                   0                        0   
62249                          0                   0                        0   
...                          ...                 ...                      ...   
236131                         0                   0                        0   
127145                         0                   0                        0   
20536                          0                   0                        0   
18709                          0                   0                        0   
35767                          0                   0                        0   

        District_Name_AHMEDNAGAR  District_Name_AIZAWL  ...  Crop_Turmeric  \
201072                         0                     0  ...              0   
191897                         0                     0  ...              0   
43814                          0                     0  ...              0   
32815                          0                     0  ...              0   
62249                          0                     0  ...              0   
...                          ...                   ...  ...            ...   
236131                         0                     0  ...              0   
127145                         0                     0  ...              0   
20536                          0                     0  ...              0   
18709                          0                     0  ...              0   
35767                          0                     0  ...              0   

        Crop_Turnip  Crop_Urad  Crop_Varagu  Crop_Water Melon  Crop_Wheat  \
201072            0          0            0                 0           0   
191897            0          0            0                 0           0   
43814             0          0            0                 0           0   
32815             0          0            0                 0           0   
62249             0          0            0                 0           0   
...             ...        ...          ...               ...         ...   
236131            0          0            0                 0           0   
127145            0          0            0                 0           0   
20536             0          0            0                 0           0   
18709             0          0            0                 0           0   
35767             0          0            0                 0           0   

        Crop_Yam  Crop_other fibres  Crop_other misc. pulses  \
201072         0                  0                        0   
191897         0                  0                        0   
43814          0                  0                        0   
32815          0                  0                        0   
62249          0                  0                        0   
...          ...                ...                      ...   
236131         0                  0                        0   
127145         0                  0                        0   
20536          0                  0                        0   
18709          0                  0                        0   
35767          0                  0                        0   

        Crop_other oilseeds  
201072                    0  
191897                    0  
43814                     0  
32815                     0  
62249                     0  
...                     ...  
236131                    0  
127145                    0  
20536                     0  
18709                     0  
35767                     0  

[181770 rows x 778 columns]
201072      11.0
191897    2000.0
43814     2555.0
32815      175.0
62249     1400.0
           ...  
236131     139.0
127145      44.0
20536       27.0
18709     4779.0
35767       81.0
Name: Production, Length: 181770, dtype: float64

Linear Regression
# Training the Simple Linear Regression model .

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)
LinearRegression()
# Predicting the test Results 

lr_predict = model.predict(x_test)
lr_predict
array([  -85149.09375 ,   477482.78125 , -1271634.28125 , ...,
         145619.703125, -1749282.734375,   119897.      ])
model.score(x_test,y_test)
-66175.59283970056
from sklearn.metrics import r2_score
r = r2_score(y_test,lr_predict)
print("R2 score : ",r)
R2 score :  -66175.59283970056
plt.scatter(y_test,lr_predict)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression')
Text(0.5, 1.0, 'Linear Regression')

Clearly, the dataset is not good for linear regression.

Assumptions of Linear Regression

Linearity.
Homoscedasticity
Multivariate normality
Lack of multicollinearity
R2 score: This is pronounced as R-squared, and this score refers to the coefficient of determination.
This tells us how well the unknown samples will be predicted by our model.

Random Forest Algorithm
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 11)
model.fit(x_train,y_train)
rf_predict = model.predict(x_test)
rf_predict
array([ 4087.93636364,   602.36363636,  2216.45454545, ...,
         220.90909091, 12160.63636364,   118.18181818])
model.score(x_test,y_test)
0.9543893048576644
# Calculating R2 score

from sklearn.metrics import r2_score
r1 = r2_score(y_test,rf_predict)
print("R2 score : ",r1)
R2 score :  0.9473978231931719
# Calculating Adj. R2 score: 

Adjr2_1 = 1 - (1-r)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)
print("Adj. R-Squared : {}".format(Adjr2_1))
Adj. R-Squared : 0.9579747198113833
ax = sns.distplot(y_test, hist = False, color = "r", label = "Actual value ")
sns.distplot(rf_predict, hist = False, color = "b", label = "Predicted Values", ax = ax)
plt.title('Random Forest Regression')
Text(0.5, 1.0, 'Random Forest Regression')

Comparison between Linear Regression Algorithm and Random Forest Algorithm

Linear regression algorithm is not at all accurate for this kind of prediction.
Random Forest Algorithm has higher accuracy ( between 85 % to 90% ), but it is slow.
Support Vector Regression

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc_x.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
print(x_train)
print(x_test)
# Training the SVR model 

from sklearn.svm import SVR 
regressor = SVR(kernel = 'rbf')
regressor.fit(x_train,y_train)
# Predicting Result

svr_predict = regressor.predict(x_test)
svr_predict
ax = sns.distplot(y_test, hist = False, color = "r", label = "Actual value ")
sns.distplot(svr_predict, hist = False, color = "b", label = "Predicted Values", ax = ax)
plt.title('Support Vector Regression')
Decision Tree

# Training model 
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 5)
regressor.fit(x_train,y_train)

# Predicting results
decisiontree_predict = regressor.predict(x_test)
decisiontree_predict
array([3900.,  705., 2377., ...,   84., 9656.,  100.])
regressor.score(x_test,y_test)
0.9585143413328677
# Calculating R2 score :

from sklearn.metrics import r2_score
r2 = r2_score(y_test,decisiontree_predict)
print("R2 score : ",r2)
R2 score :  0.9585143413328677
# Calculating Adj. R2 score: 

Adjr2_2 = 1 - (1-r)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)
print("Adj. R-Squared : {}".format(Adjr2_2))
Adj. R-Squared : 0.9579747198113833
ax = sns.distplot(y_test, hist = False, color = "r", label = "Actual value ")
sns.distplot(decisiontree_predict, hist = False, color = "b", label = "Predicted Values", ax = ax)
plt.title('Decision Tree Regression')
Text(0.5, 1.0, 'Decision Tree Regression')

Cross-validation

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = model, X = x_train, y=y_train, cv = 10)
a1 = (accuracies.mean()*100)
b1 = (accuracies.std()*100)
# Mean Accuracy and SD of 10 fold results

print("Accuracy : {:.2f}%".format (accuracies.mean()*100))
print("Standard Deviation : {:.2f}%".format(accuracies.std()*100))
Accuracy : 90.47%
Standard Deviation : 6.36%
Cross-validation

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = regressor, X = x_train, y=y_train)
a2 = (accuracies.mean()*100)
b2 = (accuracies.std()*100)
print("Accuracy : {:.2f}%".format (accuracies.mean()*100))
print("Standard Deviation : {:.2f}%".format(accuracies.std()*100))
Accuracy : 90.47%
Standard Deviation : 6.36%
import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [a1, a2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Accuracy(in %)')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()

import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [b1, b2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Standard Deviation(in %)')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

<Figure size 432x288 with 0 Axes>
import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [Adjr2_1, Adjr2_2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Standard Deviation(in %)')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

<Figure size 432x288 with 0 Axes>
import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [r1, r2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('R-Squared Score')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

<Figure size 432x288 with 0 Axes>
import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [Adjr2_1, Adjr2_2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Adjusted R-Squared Score')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

<Figure size 432x288 with 0 Axes>
mae = metrics.mean_absolute_error(y_test, y_pred)
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(rf_predict,y_test))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, rf_predict))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rf_predict)))
Mean Absolute Error: 119936.04867395548
Mean Squared Error: 15129939409623.521
Root Mean Squared Error: 3889722.279240964
import numpy as np
import matplotlib.pyplot as plt
 
# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(8, 5))
 
# set height of bar
Algorithms = ['Random Forest', 'Decision-tree']
Accuracy = [a1, a2]
Standard_Deviation = [b1,b2]
 
# Set position of bar on X axis
br1 = np.arange(len(Accuracy))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, Accuracy, color ='blue', width = barWidth,
        edgecolor ='grey', label ='Accuracy')
plt.bar(br2, Standard_Deviation, color ='maroon', width = barWidth,
        edgecolor ='grey', label ='Standard Devation')
 
# Adding Xticks
plt.xlabel('Algorithms', fontweight ='bold', fontsize = 10)
plt.ylabel('Accuracy (in %)', fontweight ='bold', fontsize = 10)
plt.xticks([r + barWidth for r in range(len(Accuracy))],
        Algorithms)
 
plt.legend()
plt.show()
Hyperparameter Tuning using GridSearchCV

Random Forest Regression

from sklearn.model_selection import GridSearchCV

# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100],  
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 
              'gamma':['auto'],
              'kernel': ['rbf','linear']}  
   
reg = GridSearchCV(DecisionTreeRegressor(), param_grid, refit = True, verbose = 3,n_jobs=-1) 
   
reg.fit(x_train,y_train)
reg.grid_scores_
Fitting 5 folds for each of 8 candidates, totalling 40 fits
---------------------------------------------------------------------------
_RemoteTraceback                          Traceback (most recent call last)
_RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py", line 431, in _process_worker
    r = call_item()
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py", line 285, in __call__
    return self.fn(*self.args, **self.kwargs)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py", line 595, in __call__
    return self.func(*args, **kwargs)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/joblib/parallel.py", line 252, in __call__
    return [func(*args, **kwargs)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/joblib/parallel.py", line 252, in <listcomp>
    return [func(*args, **kwargs)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py", line 222, in __call__
    return self.function(*args, **kwargs)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py", line 581, in _fit_and_score
    estimator = estimator.set_params(**cloned_parameters)
  File "/home/ankita/anaconda3/lib/python3.8/site-packages/sklearn/base.py", line 230, in set_params
    raise ValueError('Invalid parameter %s for estimator %s. '
ValueError: Invalid parameter C for estimator DecisionTreeRegressor(). Check the list of available parameters with `estimator.get_params().keys()`.
"""

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
<ipython-input-37-5167732e5278> in <module>
      9 reg = GridSearchCV(DecisionTreeRegressor(), param_grid, refit = True, verbose = 3,n_jobs=-1)
     10 
---> 11 reg.fit(x_train,y_train)
     12 reg.grid_scores_

~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args <= 0:
---> 63                 return f(*args, **kwargs)
     64 
     65             # extra_args > 0

~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)
    839                 return results
    840 
--> 841             self._run_search(evaluate_candidates)
    842 
    843             # multimetric is determined here because in the case of a callable

~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py in _run_search(self, evaluate_candidates)
   1286     def _run_search(self, evaluate_candidates):
   1287         """Search all candidates in param_grid"""
-> 1288         evaluate_candidates(ParameterGrid(self.param_grid))
   1289 
   1290 

~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py in evaluate_candidates(candidate_params, cv, more_results)
    793                               n_splits, n_candidates, n_candidates * n_splits))
    794 
--> 795                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
    796                                                        X, y,
    797                                                        train=train, test=test,

~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable)
   1040 
   1041             with self._backend.retrieval_context():
-> 1042                 self.retrieve()
   1043             # Make sure that we get a last message telling us we are done
   1044             elapsed_time = time.time() - self._start_time

~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self)
    919             try:
    920                 if getattr(self._backend, 'supports_timeout', False):
--> 921                     self._output.extend(job.get(timeout=self.timeout))
    922                 else:
    923                     self._output.extend(job.get())

~/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py in wrap_future_result(future, timeout)
    540         AsyncResults.get from multiprocessing."""
    541         try:
--> 542             return future.result(timeout=timeout)
    543         except CfTimeoutError as e:
    544             raise TimeoutError from e

~/anaconda3/lib/python3.8/concurrent/futures/_base.py in result(self, timeout)
    437                 raise CancelledError()
    438             elif self._state == FINISHED:
--> 439                 return self.__get_result()
    440             else:
    441                 raise TimeoutError()

~/anaconda3/lib/python3.8/concurrent/futures/_base.py in __get_result(self)
    386     def __get_result(self):
    387         if self._exception:
--> 388             raise self._exception
    389         else:
    390             return self._result

ValueError: Invalid parameter C for estimator DecisionTreeRegressor(). Check the list of available parameters with `estimator.get_params().keys()`.
# CV results are not easy to use, 
# sklearn provides a way to download these results into a dataframe 
df = pd.DataFrame(reg.cv_results_)
df
df[['param_C','param_kernel','mean_test_score']]
reg.best_score_
reg.best_params_
# To tackle the computation problem in gridsearch , 
# randomizedsearchcv comes in. Randomly tries value.
from sklearn.model_selection import RandomizedSearchCV
rs = RandomizedSearchCV(regressor(gamma='auto'),{
    'c': [1,10,20],
    'kernel' : ['rbf','linear']
},
 cv = 5,
 return_train_score=False,
 n_iter=2
)
rs.fit(x_train,y_train)
pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-39-e0f74b0310de> in <module>
      1 from sklearn.model_selection import RandomizedSearchCV
----> 2 rs = RandomizedSearchCV(decisiontree_predict(gamma='auto'),{
      3     'c': [1,10,20],
      4     'kernel' : ['rbf','linear']
      5 },

TypeError: 'numpy.ndarray' object is not callable
Decision Tree

 
 
